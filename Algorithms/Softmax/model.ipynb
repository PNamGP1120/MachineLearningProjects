{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T08:56:43.688609Z",
     "start_time": "2025-02-24T08:56:43.685178Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:56:44.678932Z",
     "start_time": "2025-02-24T08:56:44.675516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute the softmax function.\n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Stability trick\n",
    "    return e_Z / e_Z.sum(axis=1, keepdims=True)"
   ],
   "id": "d5597de99c911de2",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:56:52.095566Z",
     "start_time": "2025-02-24T08:56:52.092090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(X, y, W):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss.\n",
    "    \"\"\"\n",
    "    A = softmax(X.dot(W))\n",
    "    id0 = np.arange(X.shape[0])\n",
    "    return -np.mean(np.log(A[id0, y]))"
   ],
   "id": "91b3b90ad989b9e3",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:56:53.603242Z",
     "start_time": "2025-02-24T08:56:53.600006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(X, y, W):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss.\n",
    "    \"\"\"\n",
    "    A = softmax(X.dot(W))\n",
    "    id0 = np.arange(X.shape[0])\n",
    "    return -np.mean(np.log(A[id0, y]))"
   ],
   "id": "cc3994ac579748ad",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:56:57.610999Z",
     "start_time": "2025-02-24T08:56:57.606530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fit(X, y, learning_rate=0.01, epochs=100, tol=1e-5, batch_size=10):\n",
    "    \"\"\"\n",
    "    Train the Softmax Regression model using mini-batch gradient descent.\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    C = np.max(y) + 1  # Number of classes\n",
    "    W = np.random.randn(d, C)\n",
    "    W_old = W.copy()\n",
    "    loss_hist = [compute_loss(X, y, W)]\n",
    "\n",
    "    nbatches = int(np.ceil(N / batch_size))\n",
    "    for ep in range(epochs):\n",
    "        mix_ids = np.random.permutation(N)\n",
    "        for i in range(nbatches):\n",
    "            batch_ids = mix_ids[batch_size * i: min(batch_size * (i + 1), N)]\n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            W -= learning_rate * compute_gradient(X_batch, y_batch, W)\n",
    "        loss_hist.append(compute_loss(X, y, W))\n",
    "\n",
    "        if np.linalg.norm(W - W_old) / W.size < tol:\n",
    "            break\n",
    "        W_old = W.copy()\n",
    "\n",
    "    return W, loss_hist"
   ],
   "id": "934621532cd06284",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:57:00.002420Z",
     "start_time": "2025-02-24T08:56:59.998942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Predict class labels for given input data.\n",
    "    \"\"\"\n",
    "    return np.argmax(X.dot(W), axis=1)"
   ],
   "id": "caab6d82dc12cc42",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:57:01.814422Z",
     "start_time": "2025-02-24T08:57:01.807487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "C, N = 5, 500\n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "\n",
    "X = np.vstack([np.random.multivariate_normal(m, cov, N) for m in means])\n",
    "y = np.hstack([[i] * N for i in range(C)])\n",
    "\n",
    "Xbar = np.hstack((X, np.ones((X.shape[0], 1))))  # Thêm bias\n"
   ],
   "id": "d36669949bf9deab",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:57:03.768746Z",
     "start_time": "2025-02-24T08:57:03.749584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W, loss_history = fit(Xbar, y)\n",
    "print(\"Trọng số cuối cùng của mô hình:\\n\", W)\n"
   ],
   "id": "89d224494e143e36",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_gradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m W, loss_history \u001B[38;5;241m=\u001B[39m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXbar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrọng số cuối cùng của mô hình:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, W)\n",
      "Cell \u001B[1;32mIn[48], line 17\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(X, y, learning_rate, epochs, tol, batch_size)\u001B[0m\n\u001B[0;32m     15\u001B[0m     batch_ids \u001B[38;5;241m=\u001B[39m mix_ids[batch_size \u001B[38;5;241m*\u001B[39m i: \u001B[38;5;28mmin\u001B[39m(batch_size \u001B[38;5;241m*\u001B[39m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), N)]\n\u001B[0;32m     16\u001B[0m     X_batch, y_batch \u001B[38;5;241m=\u001B[39m X[batch_ids], y[batch_ids]\n\u001B[1;32m---> 17\u001B[0m     W \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m \u001B[43mcompute_gradient\u001B[49m(X_batch, y_batch, W)\n\u001B[0;32m     18\u001B[0m loss_hist\u001B[38;5;241m.\u001B[39mappend(compute_loss(X, y, W))\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(W \u001B[38;5;241m-\u001B[39m W_old) \u001B[38;5;241m/\u001B[39m W\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m<\u001B[39m tol:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'compute_gradient' is not defined"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred = predict(Xbar, W)\n",
   "id": "2561b21588bc4884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "new_point = np.array([[7, 5]])  # Điểm mới cần dự đoán\n",
    "new_point_bar = np.hstack((new_point, np.ones((new_point.shape[0], 1))))  # Thêm bias\n",
    "y_new_pred = predict(new_point_bar, W)\n",
    "print(f\"Điểm mới {new_point} được dự đoán thuộc lớp: {y_new_pred[0]}\")\n"
   ],
   "id": "6a590bf4cfe729d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "colors = ['r', 'g', 'b', 'c', 'm']\n",
    "for i in range(C):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], c=colors[i], label=f'Class {i}', alpha=0.6)\n",
    "\n",
    "x_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "for i in range(C):\n",
    "    slope = -W[0, i] / W[1, i]\n",
    "    intercept = -W[2, i] / W[1, i]\n",
    "    y_vals = slope * x_vals + intercept\n",
    "    plt.plot(x_vals, y_vals, linestyle='--', label=f'Decision boundary {i}')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundaries of Softmax Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "e01d14ba313f48d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "N_new = 100\n",
    "X_new = np.vstack([np.random.multivariate_normal(m, cov, N_new // C) for m in means])\n",
    "y_new = np.hstack([[i] * (N_new // C) for i in range(C)])\n",
    "\n",
    "X_new_bar = np.hstack((X_new, np.ones((X_new.shape[0], 1))))\n",
    "\n",
    "y_new_pred = predict(X_new_bar, W)\n",
    "\n",
    "accuracy_new = np.mean(y_new_pred == y_new) * 100\n",
    "print(f\"Độ chính xác trên tập dữ liệu mới: {accuracy_new:.2f}%\")\n"
   ],
   "id": "7dc8e76d370858f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(C):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], c=colors[i], label=f'Class {i} (train)', alpha=0.4)\n",
    "\n",
    "for i in range(C):\n",
    "    plt.scatter(X_new[y_new_pred == i, 0], X_new[y_new_pred == i, 1], edgecolors='k', facecolors='none', s=100, label=f'Class {i} (new)')\n",
    "\n",
    "x_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "for i in range(C):\n",
    "    slope = -W[0, i] / W[1, i]\n",
    "    intercept = -W[2, i] / W[1, i]\n",
    "    y_vals = slope * x_vals + intercept\n",
    "    plt.plot(x_vals, y_vals, linestyle='--', label=f'Decision boundary {i}')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(f'Softmax Regression with New Data\\nAccuracy: {accuracy_new:.2f}%')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "9d25a75ec78a95db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
